{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Maths\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistics\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import binom\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlined Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HistoricalDataPreProcessing\n",
    "processed_data = HistoricalDataPreProcessing.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RelativeDifferenceTools import RelativeDifferenceCalculator, RelativeDifferenceSampleGenerator, VehicleEstimator\n",
    "\n",
    "relative_difference_calculator = RelativeDifferenceCalculator(\n",
    "    processed_data['car_van_2011'],\n",
    "    processed_data['car_van_2021'],\n",
    "    processed_data['vehicle_registrations']\n",
    ")\n",
    "relative_difference_data = relative_difference_calculator.calculate()\n",
    "relative_difference_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_difference_sample_generator = RelativeDifferenceSampleGenerator(\n",
    "    relative_difference_data=relative_difference_data,\n",
    "    n_samples=1000\n",
    ")\n",
    "relative_difference_samples = relative_difference_sample_generator.generate_samples()\n",
    "relative_difference_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_estimator = VehicleEstimator(relative_difference_samples)\n",
    "\n",
    "vehicle_samples = vehicle_estimator.estimate(processed_data['vehicle_registrations_i'], '2023 Q1')\n",
    "bev_samples = vehicle_estimator.estimate(processed_data['bev_registrations_i'], '2023 Q1')\n",
    "phev_samples = vehicle_estimator.estimate(processed_data['phev_registrations_i'], '2023 Q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataContainer:\n",
    "    file_path: str\n",
    "    raw_data: pd.DataFrame = None\n",
    "    data: pd.DataFrame = None\n",
    "\n",
    "class BasePreprocessor():\n",
    "    def __init__(self, data_container: DataContainer) -> None:\n",
    "        self.data_container = data_container\n",
    "    \n",
    "    def preprocess(self, keep_raw: bool, dtype=None, na_values=None) -> None:\n",
    "        self._load_csv_to_df(self.data_container, keep_raw, dtype, na_values)\n",
    "\n",
    "    def _load_csv_to_df(self, container: DataContainer, keep_raw: bool, dtype=None, na_values=None) -> None:\n",
    "        try:\n",
    "            container.data = pd.read_csv(container.file_path, dtype=dtype, na_values=na_values)\n",
    "            if keep_raw:\n",
    "                container.raw_data = container.data.copy()\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"File not found: {container.file_path}\") from e\n",
    "    \n",
    "    def _rename_df_columns(self, column_mapper: dict) -> None:\n",
    "        self.data_container.data = self.data_container.data.rename(columns=column_mapper)\n",
    "\n",
    "    def _select_df_columns(self, columns: list) -> None:\n",
    "        self.data_container.data = self.data_container.data[columns]\n",
    "\n",
    "    def _drop_duplicate_rows(self, subset) -> None:\n",
    "        self.data_container.data.drop_duplicates(subset=subset, keep='first', inplace=True)\n",
    "    \n",
    "    def _drop_duplicate_rows_by_index(self) -> None:\n",
    "        self.data_container.data = self.data_container.data[~self.data_container.data.index.duplicated(keep='first')]\n",
    "\n",
    "    def _set_df_index(self, index_name: str, drop: bool) -> None:\n",
    "        self.data_container.data = self.data_container.data.set_index(index_name, drop=drop)\n",
    "    \n",
    "    def _apply_dtypes(self, start_col: int, end_col: int) -> dict:\n",
    "        dtypes = {i: str for i in range(start_col)} # first 'first' columns as strings\n",
    "        dtypes.update({i: float for i in range(start_col, end_col)}) # 'last' columns is currently hard coded. Float is needed for NaNs\n",
    "        return dtypes\n",
    "\n",
    "class CarVan2011DataPreprocessor(BasePreprocessor):\n",
    "    def preprocess(self, keep_raw: bool) -> pd.DataFrame:\n",
    "        super().preprocess(keep_raw)\n",
    "        self._rename_df_columns({\n",
    "            'GEO_CODE': 'LSOA11CD',\n",
    "            'GEO_LABEL': 'LSOA11NM',\n",
    "            'Car or van availability : Sum of all cars or vans - Unit : Cars or vans': 'cars',\n",
    "            'Car or van availability : No cars or vans in household - Unit : Households': 'households_without_cars',\n",
    "            'Car or van availability : Total\\ Car or van availability - Unit : Households': 'households'\n",
    "            }\n",
    "        )\n",
    "        self._drop_duplicate_rows('LSOA11CD')\n",
    "        self._set_df_index('LSOA11CD', drop=True)\n",
    "        self._select_df_columns(['LSOA11NM', 'cars', 'households', 'households_without_cars'])\n",
    "        self._drop_duplicate_rows_by_index()\n",
    "        print('Pre-processing complete')\n",
    "        return self.data_container.data\n",
    "\n",
    "class CarVan2021DataPreprocessor(BasePreprocessor):  \n",
    "    def __init__(self, data_container: DataContainer, lsoa_lookup_file_name: str) -> None:\n",
    "        super().__init__(data_container)\n",
    "        self.lsoa_lookup_file_name = lsoa_lookup_file_name \n",
    "\n",
    "    def preprocess(self, keep_raw: bool) -> pd.DataFrame:\n",
    "        super().preprocess(keep_raw)\n",
    "        self._count_number_of_cars()\n",
    "        self._condense_data()\n",
    "        self._reindex_data()\n",
    "        self._drop_duplicate_rows_by_index()\n",
    "        print('Pre-processing complete')\n",
    "        return self.data_container.data\n",
    "\n",
    "    def _count_number_of_cars(self) -> None:\n",
    "        self.data_container.raw_data['cars'] = self.data_container.raw_data['Observation'] * self.data_container.raw_data['Car or van availability (5 categories) Code']\n",
    "\n",
    "    def _condense_data(self) -> None:\n",
    "        self.data_container.data = (\n",
    "            pd.DataFrame(index=self.data_container.raw_data['Lower Layer Super Output Areas Code'].unique(), columns=['LSOA21CD', 'LSOA21NM', 'cars', 'houses_without_cars'])\n",
    "            .assign(\n",
    "                LSOA21CD=lambda df: df.index, \n",
    "                LSOA21NM=self.data_container.raw_data['Lower Layer Super Output Areas'].unique(),\n",
    "                cars=self.data_container.raw_data.groupby('Lower Layer Super Output Areas Code')['cars'].sum(),\n",
    "                houses_without_cars=self.data_container.raw_data.loc[self.data_container.raw_data['Car or van availability (5 categories) Code'] == 0, ['Lower Layer Super Output Areas Code', 'Observation']].set_index('Lower Layer Super Output Areas Code')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _reindex_data(self) -> None:\n",
    "        lsoa_lookup = pd.read_csv(self.lsoa_lookup_file_name)\n",
    "        data = self.data_container.data\n",
    "        self.data_container.data = (\n",
    "            data\n",
    "            .merge(lsoa_lookup[['LSOA11CD', 'LSOA21CD']], on='LSOA21CD', how='right')\n",
    "            .assign(cars=lambda df: df.groupby('LSOA11CD')['cars'].transform('sum'))\n",
    "            .drop(columns=['LSOA21CD'])\n",
    "        )\n",
    "        self._set_df_index('LSOA11CD', drop=True)\n",
    "\n",
    "class VehicleRegistrationsDataPreprocessor(BasePreprocessor):\n",
    "    def __init__(self, data_container: DataContainer):\n",
    "        super().__init__(data_container)\n",
    "        self.data_container = data_container\n",
    "    \n",
    "    def preprocess(self, keep_raw: bool) -> pd.DataFrame:\n",
    "        super().preprocess(keep_raw, dtype=self._apply_dtypes(5, 57), na_values=['[c]', '[x]'])\n",
    "        self.data_container.data = self._preprocess_by_bodytype('Cars') + self._preprocess_by_bodytype('Other body types')\n",
    "        self._drop_duplicate_rows_by_index()\n",
    "        self.data_container.data = self.data_container.data.drop('Miscellaneous')\n",
    "        self.data_container.data = self.data_container.data.dropna(how='all')\n",
    "        print('Pre-processing complete')\n",
    "        return self.data_container.data\n",
    "    \n",
    "    def _preprocess_by_bodytype(self, bodytype: str) -> pd.DataFrame:\n",
    "        bodytype_data = self.data_container.data.query(\"BodyType == '\" + bodytype + \"' & Keepership == 'Private' & LicenceStatus == 'Licensed'\")\n",
    "        bodytype_data = bodytype_data.drop(columns = ['BodyType', 'Keepership', 'LicenceStatus', 'LSOA11NM'])\n",
    "        bodytype_data = bodytype_data.set_index('LSOA11CD', drop=True)\n",
    "        return bodytype_data\n",
    "    \n",
    "class EVRegistrationsDataPreprocessor(BasePreprocessor):\n",
    "    def __init__(self, data_container: DataContainer, fuel_type: str):\n",
    "        super().__init__(data_container)\n",
    "        self.data_container = data_container\n",
    "        self.fuel_type = fuel_type\n",
    "    \n",
    "    def preprocess(self, keep_raw: bool) -> pd.DataFrame:\n",
    "        super().preprocess(keep_raw, dtype=self._apply_dtypes(4, 56), na_values=['[c]', '[x]'])\n",
    "        self._filter_data()\n",
    "        self._set_df_index('LSOA11CD', drop=True)\n",
    "        self._split_by_fuel_type()\n",
    "        self._drop_duplicate_rows_by_index()\n",
    "        print('Pre-processing complete')\n",
    "        return self.data_container.data\n",
    "    \n",
    "    def _filter_data(self):\n",
    "        self.data_container.data = (\n",
    "            self.data_container.data\n",
    "            .query(\"Keepership == 'Private'\")\n",
    "            .drop(columns=['Keepership', 'LSOA11NM'])\n",
    "        )\n",
    "    \n",
    "    def _split_by_fuel_type(self):\n",
    "        self.data_container.data = (\n",
    "            self.data_container.data\n",
    "            .query(\"Fuel == '\" + self.fuel_type + \"' or Fuel.isnull()\")\n",
    "            .drop(columns=[\"Fuel\"])\n",
    "        )\n",
    "\n",
    "class HouseDataPreprocessor(BasePreprocessor):\n",
    "    def __init__(self, data_container: DataContainer, lsoa_lookup_file_name: str) -> None:\n",
    "        super().__init__(data_container)\n",
    "        self.lsoa_lookup_file_name = lsoa_lookup_file_name \n",
    "    \n",
    "    def preprocess(self, keep_raw: bool, dtype=None, na_values=None) -> pd.DataFrame:\n",
    "        super().preprocess(keep_raw, dtype, na_values)\n",
    "        lsoa_lookup = pd.read_csv(self.lsoa_lookup_file_name)\n",
    "        self.data_container.data = (\n",
    "            self.data_container.data\n",
    "            .rename(columns={'Lower layer Super Output Areas Code':'LSOA21CD', 'Lower layer Super Output Areas':'LSOA21NM', 'Observation':'households'})\n",
    "            .merge(lsoa_lookup.loc[:, ['LSOA11CD', 'LSOA21CD']], on = 'LSOA21CD', how='outer')\n",
    "            .drop(columns=['LSOA21NM'])\n",
    "            .set_index('LSOA11CD')\n",
    "        )\n",
    "        self._drop_duplicate_rows_by_index()\n",
    "        print('Pre-processing complete')\n",
    "        return self.data_container.data\n",
    "\n",
    "class AccommodationTypeDataPreprocessor(BasePreprocessor):\n",
    "    def __init__(self, data_container: DataContainer, lsoa_lookup_file_name: str) -> None:\n",
    "        super().__init__(data_container)\n",
    "        self.lsoa_lookup_file_name = lsoa_lookup_file_name \n",
    "    \n",
    "    def preprocess(self, keep_raw: bool, dtype=None, na_values=None) -> pd.DataFrame:\n",
    "        super().preprocess(keep_raw, dtype, na_values)\n",
    "        lsoa_lookup = pd.read_csv(self.lsoa_lookup_file_name)\n",
    "        self.data_container.data = (\n",
    "            self.data_container.data\n",
    "            .rename(columns={'Lower layer Super Output Areas Code':'LSOA21CD', 'Lower layer Super Output Areas':'LSOA21NM', 'Accommodation type (8 categories)':'accommodation_type'})\n",
    "            .merge(lsoa_lookup.loc[:, ['LSOA11CD', 'LSOA21CD']], on = 'LSOA21CD', how='outer')\n",
    "            .drop(columns=['LSOA21NM', 'Accommodation type (8 categories) Code'])\n",
    "            .set_index('LSOA11CD')\n",
    "        )\n",
    "        print('Pre-processing complete')\n",
    "        return self.data_container.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Containers\n",
    "car_van_2011_container = DataContainer(file_path='../Data/Vehicle/CarsAndVans2011.csv')\n",
    "car_van_2021_container = DataContainer(file_path='../Data/Vehicle/CarsAndVans2021.csv')\n",
    "vehicle_registrations_container = DataContainer(file_path='../Data/Vehicle/df_VEH0125.csv')\n",
    "bev_registrations_container = DataContainer(file_path='../Data/Vehicle/df_VEH0145.csv')\n",
    "phev_registrations_container = DataContainer(file_path='../Data/Vehicle/df_VEH0145.csv')\n",
    "house_2021_container = DataContainer(file_path='../Data/Demographic/LSOA_households.csv')\n",
    "accomodation_type_2021_container = DataContainer(file_path='../Data/Demographic/LSOA_accommodation_type.csv')\n",
    "\n",
    "# Load and preprocess data\n",
    "lsoa_lookup_file = '../Data/Spatial/LSOA/LSOA_(2011)_to_LSOA_(2021)_to_Local_Authority_District_(2022)_Lookup_for_England_and_Wales_(Version_2).csv'\n",
    "car_van_2011_data_preprocessor = CarVan2011DataPreprocessor(car_van_2011_container)\n",
    "car_van_2021_data_preprocessor = CarVan2021DataPreprocessor(car_van_2021_container, lsoa_lookup_file)\n",
    "vehicle_registrations_data_preprocessor = VehicleRegistrationsDataPreprocessor(vehicle_registrations_container)\n",
    "bev_registrations_data_preprocessor = EVRegistrationsDataPreprocessor(bev_registrations_container, fuel_type='Battery electric')\n",
    "phev_registrations_data_preprocessor = EVRegistrationsDataPreprocessor(phev_registrations_container, fuel_type='Plug-in hybrid electric (petrol)')\n",
    "house_2021_data_preprocessor = HouseDataPreprocessor(house_2021_container, lsoa_lookup_file)\n",
    "accomodation_type_2021_data_preprocessor = AccommodationTypeDataPreprocessor(accomodation_type_2021_container, lsoa_lookup_file)\n",
    "\n",
    "car_van_2011_data = car_van_2011_data_preprocessor.preprocess(keep_raw=False)\n",
    "car_van_2021_data = car_van_2021_data_preprocessor.preprocess(keep_raw=True)\n",
    "vehicle_registrations_data = vehicle_registrations_data_preprocessor.preprocess(keep_raw=False)\n",
    "bev_registrations_data = bev_registrations_data_preprocessor.preprocess(keep_raw=True)\n",
    "phev_registrations_data = phev_registrations_data_preprocessor.preprocess(keep_raw=True)\n",
    "house_2021_data = house_2021_data_preprocessor.preprocess(keep_raw=False)\n",
    "accomodation_type_2021_data = accomodation_type_2021_data_preprocessor.preprocess(keep_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "file_paths = {\n",
    "    'car_van_2011': '../Data/Vehicle/CarsAndVans2011.csv',\n",
    "    'car_van_2021': '../Data/Vehicle/CarsAndVans2021.csv',\n",
    "    'vehicle_registrations': '../Data/Vehicle/df_VEH0125.csv',\n",
    "    'bev_registrations': '../Data/Vehicle/df_VEH0145.csv',\n",
    "    'phev_registrations': '../Data/Vehicle/df_VEH0145.csv',\n",
    "    'house_2021': '../Data/Demographic/LSOA_households.csv',\n",
    "    'accomodation_type_2021': '../Data/Demographic/LSOA_accommodation_type.csv'\n",
    "}\n",
    "\n",
    "# Create Data Containers\n",
    "data_containers = {key: DataContainer(file_path=value) for key, value in file_paths.items()}\n",
    "\n",
    "# Load and preprocess data\n",
    "lsoa_lookup_file = '../Data/Spatial/LSOA/LSOA_(2011)_to_LSOA_(2021)_to_Local_Authority_District_(2022)_Lookup_for_England_and_Wales_(Version_2).csv'\n",
    "\n",
    "data_preprocessors = {\n",
    "    'car_van_2011': CarVan2011DataPreprocessor(data_containers['car_van_2011']),\n",
    "    'car_van_2021': CarVan2021DataPreprocessor(data_containers['car_van_2021'], lsoa_lookup_file),\n",
    "    'vehicle_registrations': VehicleRegistrationsDataPreprocessor(data_containers['vehicle_registrations']),\n",
    "    'bev_registrations': EVRegistrationsDataPreprocessor(data_containers['bev_registrations'], fuel_type='Battery electric'),\n",
    "    'phev_registrations': EVRegistrationsDataPreprocessor(data_containers['phev_registrations'], fuel_type='Plug-in hybrid electric (petrol)'),\n",
    "    'house_2021': HouseDataPreprocessor(data_containers['house_2021'], lsoa_lookup_file),\n",
    "    'accomodation_type_2021': AccommodationTypeDataPreprocessor(data_containers['accomodation_type_2021'], lsoa_lookup_file)\n",
    "}\n",
    "\n",
    "# Get processed data\n",
    "keep_raw_values = {\n",
    "    'car_van_2011': False,\n",
    "    'car_van_2021': True,\n",
    "    'vehicle_registrations': False,\n",
    "    'bev_registrations': True,\n",
    "    'phev_registrations': True,\n",
    "    'house_2021': False,\n",
    "    'accomodation_type_2021': False\n",
    "}\n",
    "\n",
    "processed_data = {key: processor.preprocess(keep_raw=keep_raw_values[key]) for key, processor in data_preprocessors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['car_van_2011']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_van_2021_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_registrations_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Common LSOA Codes and Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListUtilities:\n",
    "    \"\"\"\n",
    "    A utility class for list operations.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def intersection_of_lists(*args):\n",
    "        \"\"\"\n",
    "        Returns the intersection of all provided lists.\n",
    "\n",
    "        Args:\n",
    "        *args: Variable length argument of lists.\n",
    "\n",
    "        Returns:\n",
    "        List containing the intersection of all provided lists.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If no lists are provided, return an empty list\n",
    "        if not args:\n",
    "            return []\n",
    "\n",
    "        # Start with the set of the first list\n",
    "        result_set = set(args[0])\n",
    "\n",
    "        # Iterate over the remaining lists, updating the result_set\n",
    "        for lst in args[1:]:\n",
    "            result_set &= set(lst)\n",
    "\n",
    "        return list(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_lsoas = ListUtilities.intersection_of_lists(\n",
    "    car_van_2011_data.index,\n",
    "    car_van_2021_data.index,\n",
    "    vehicle_registrations_data.index,\n",
    "    bev_registrations_data.index,\n",
    "    phev_registrations_data.index,\n",
    "    house_2021_data.index,\n",
    "    accomodation_type_2021_data.index\n",
    ")\n",
    "\n",
    "car_van_2011_data = car_van_2011_data.loc[common_lsoas].sort_index()\n",
    "car_van_2021_data = car_van_2021_data.loc[common_lsoas].sort_index()\n",
    "vehicle_registrations_data = vehicle_registrations_data.loc[common_lsoas].sort_index()\n",
    "bev_registrations_data = bev_registrations_data.loc[common_lsoas].sort_index()\n",
    "phev_registrations_data = phev_registrations_data.loc[common_lsoas].sort_index()\n",
    "house_2021_data = house_2021_data.loc[common_lsoas].sort_index()\n",
    "accomodation_type_2021_data = accomodation_type_2021_data.loc[common_lsoas].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating EV Adoption in LSOAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegistrationInterpolator:\n",
    "    def __init__(self, sample_rate=4) -> None:\n",
    "        self.registration_data = None\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    # Interpolates missing registration data\n",
    "    def interpolate(self, registration_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.registration_data = registration_data.T.iloc[::-1]\n",
    "        interpolated_df = self.registration_data.apply(self._interpolate_column, axis=0)\n",
    "        interpolated_df = interpolated_df.fillna(0)\n",
    "        interpolated_df = interpolated_df.astype('Int64')\n",
    "        return interpolated_df.iloc[::-1].T\n",
    "    \n",
    "    def _interpolate_column(self, col) -> pd.Series:\n",
    "        dates = self._calculate_date_range(self._calculate_t0(col), self._calculate_t_present(col))\n",
    "        mask = ~col.isna().values\n",
    "        if mask.any():\n",
    "            xp = dates[mask]\n",
    "            fp = col[mask]\n",
    "            x = dates\n",
    "            interpolated_array = np.round(np.interp(x, xp, fp))\n",
    "            interpolated_series = pd.Series(data=interpolated_array, index=col.index)\n",
    "        else:\n",
    "            interpolated_series = pd.Series(data=np.nan, index=col.index)\n",
    "        return interpolated_series\n",
    "\n",
    "    # apply_dtypes converts select columns from str to float values\n",
    "    def _apply_dtypes(first_col: int, last_col: int) -> dict:\n",
    "        dtypes = {i: str for i in range(first_col)}  # first 'first' columns as strings\n",
    "        dtypes.update({i: float for i in range(first_col, last_col)}) # 'last' columns is currently hard coded. Float is needed for NaNs\n",
    "        return dtypes\n",
    "    \n",
    "    def _quarter_to_decimal(self, year: int, quarter: str) -> float:\n",
    "        \"\"\"\n",
    "        Converts a year and quarter to a decimal year representation.\n",
    "        \"\"\"\n",
    "        quarters = {'Q1': 0, 'Q2': 0.25, 'Q3': 0.5, 'Q4': 0.75}\n",
    "        return year + quarters.get(quarter, 0)\n",
    "\n",
    "    def _calculate_t0(self, col) -> float:\n",
    "        year = int(col.head(1).index[0][:4])\n",
    "        quarter = col.head(1).index[0][-2:]\n",
    "        return self._quarter_to_decimal(year, quarter)\n",
    "\n",
    "    def _calculate_t_present(self, col) -> float:\n",
    "        year = int(col.tail(1).index[0][:4])\n",
    "        quarter = col.tail(1).index[0][-2:]\n",
    "        return self._quarter_to_decimal(year, quarter)\n",
    "\n",
    "    def _convert_dates_to_numeric(self) -> list:\n",
    "        dates = []\n",
    "        for date in self.registration_data.index:\n",
    "            year = int(date[:4])\n",
    "            quarter = date[-2:]\n",
    "            dates.append(self._quarter_to_decimal(year, quarter))\n",
    "        return dates\n",
    "\n",
    "    # Returns an array of numeric dates between t0 and t1 at a specified sample rate\n",
    "    def _calculate_date_range(self, t0: float, t1: float) -> list:\n",
    "        return np.linspace(t0, t1, int((t1-t0)*self.sample_rate) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registration_interpolator = RegistrationInterpolator()\n",
    "vehicle_registrations_data_i = registration_interpolator.interpolate(vehicle_registrations_data)\n",
    "bev_registrations_data_i = registration_interpolator.interpolate(bev_registrations_data)\n",
    "phev_registrations_data_i = registration_interpolator.interpolate(phev_registrations_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Error and Uncertainty in Vehicle Registration Data (The quick way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleRegistrationRelativeDifferenceCalculator:\n",
    "    def __init__(\n",
    "            self, \n",
    "            car_van_2011_data: pd.DataFrame, \n",
    "            car_van_2021_data: pd.DataFrame, \n",
    "            vehicle_registrations_data: pd.DataFrame, \n",
    "        ):\n",
    "        self.car_van_2011_data = car_van_2011_data\n",
    "        self.car_van_2021_data = car_van_2021_data\n",
    "        self.vehicle_registrations_data = vehicle_registrations_data.astype(float)\n",
    "        self.relative_difference_data = None\n",
    "\n",
    "    def calculate(self) -> pd.DataFrame:\n",
    "        self._calculate_relative_differences(2011)\n",
    "        self._calculate_relative_differences(2021)\n",
    "        self._merge_relative_difference_data()\n",
    "        return self.relative_difference_data \n",
    "\n",
    "    def _calculate_mean_registered_vehicles_for_year(self, year: int) -> None:\n",
    "        columns = [f\"{year} {quarter}\" for quarter in ['Q1', 'Q2', 'Q3', 'Q4']]\n",
    "        df = getattr(self, f'car_van_{year}_data')\n",
    "        df['registered_vehicles'] = self.vehicle_registrations_data.loc[:, columns].mean(axis=1).round()\n",
    "    \n",
    "    def _calculate_absolute_differences(self, year: int) -> None:\n",
    "        df = getattr(self, f'car_van_{year}_data')\n",
    "        df['abs_difference'] = df['cars'] - df['registered_vehicles']\n",
    "    \n",
    "    def _calculate_relative_differences(self, year: int) -> None:\n",
    "        self._calculate_mean_registered_vehicles_for_year(year)\n",
    "        self._calculate_absolute_differences(year)\n",
    "        df = getattr(self, f'car_van_{year}_data')\n",
    "        df['relative_difference'] = df['abs_difference'] / df['registered_vehicles']\n",
    "    \n",
    "    def _merge_relative_difference_data(self) -> None:\n",
    "        relative_difference_2011_df = pd.DataFrame({'LSOA11CD': self.car_van_2011_data['relative_difference'].index, 'relative_difference_2011': self.car_van_2011_data['relative_difference'].values})\n",
    "        relative_difference_2021_df = pd.DataFrame({'LSOA11CD': self.car_van_2021_data['relative_difference'].index, 'relative_difference_2021': self.car_van_2021_data['relative_difference'].values})\n",
    "        self.relative_difference_data = pd.merge(relative_difference_2011_df, relative_difference_2021_df, how='outer', on='LSOA11CD').set_index('LSOA11CD')\n",
    "    \n",
    "class RelativeDifferenceSampleGenerator:\n",
    "    def __init__(self, relative_difference_data: pd.DataFrame, n_samples: int):\n",
    "        self.relative_difference_data = relative_difference_data\n",
    "        self.relative_difference_samples = None\n",
    "        self.n_samples = n_samples\n",
    "    \n",
    "    def generate_samples(self) -> pd.DataFrame:\n",
    "        self._calculate_relative_difference_mu_and_sigma()\n",
    "        self._calculate_relative_difference_samples()\n",
    "        return self.relative_difference_samples\n",
    "\n",
    "    def _calculate_relative_difference_mu_and_sigma(self) -> None:\n",
    "        # Priors\n",
    "        mu_prior = pd.concat([self.relative_difference_data['relative_difference_2011'], self.relative_difference_data['relative_difference_2021']]).mean()\n",
    "        sigma_prior = pd.concat([self.relative_difference_data['relative_difference_2011'], self.relative_difference_data['relative_difference_2021']]).std()\n",
    "        var_prior = sigma_prior**2\n",
    "        precision_prior = 1/var_prior\n",
    "\n",
    "        # Data\n",
    "        mu_lsoa = self.relative_difference_data[['relative_difference_2011', 'relative_difference_2021']].mean(axis=1)\n",
    "        sigma_lsoa = self.relative_difference_data[['relative_difference_2011', 'relative_difference_2021']].std(axis=1)\n",
    "        var_lsoa = sigma_lsoa**2\n",
    "        precision_lsoa = 1/var_lsoa\n",
    "\n",
    "        # Posterior\n",
    "        mu_post = (2*mu_lsoa*precision_lsoa + mu_prior*precision_prior)/(2*precision_lsoa + precision_prior)\n",
    "        sigma_post = np.sqrt(1/(2*precision_lsoa + precision_prior))\n",
    "\n",
    "        self.relative_difference_data['mu_post'] = mu_post\n",
    "        self.relative_difference_data['sigma_post'] = sigma_post\n",
    "\n",
    "        # Fill in missing data with mu_prior and sigma_prior (the mean mu and sigma for all LSOAs) if mu_post or sigma_post is NaN\n",
    "        # This is a result from missing either Census and Registration data in both 2011 and 2021\n",
    "        self.relative_difference_data.loc[self.relative_difference_data['mu_post'].isna(), 'mu_post'] = mu_lsoa[self.relative_difference_data['mu_post'].isna()]\n",
    "        self.relative_difference_data.loc[self.relative_difference_data['mu_post'].isna(), 'mu_post'] = mu_prior\n",
    "        self.relative_difference_data.loc[self.relative_difference_data['sigma_post'].isna(), 'sigma_post'] = sigma_prior\n",
    "\n",
    "    def _calculate_relative_difference_samples(self) -> None:\n",
    "        # Initialize an empty DataFrame for samples\n",
    "\n",
    "        lsoa_list = self.relative_difference_data.index.values\n",
    "        self.relative_difference_samples = pd.DataFrame(index=np.arange(0, self.n_samples), columns=lsoa_list)\n",
    "\n",
    "        # Loop over each LSOA to generate samples\n",
    "        for lsoa in lsoa_list:\n",
    "            sigma = self.relative_difference_data.sigma_post.loc[lsoa]  # Assuming sigma_post is indexed by LSOA\n",
    "            mu = self.relative_difference_data.mu_post.loc[lsoa]\n",
    "            sample_array = np.random.normal(mu, sigma, size=self.n_samples)\n",
    "            self.relative_difference_samples[lsoa] = sample_array\n",
    "\n",
    "class VehicleRegistrationEstimator:\n",
    "    def __init__(self, relative_difference_samples) -> None:\n",
    "        self.relative_difference_samples = relative_difference_samples\n",
    "    #     self.registration_data = registration_data\n",
    "    #     self.adjusted_registration_data = None\n",
    "    \n",
    "    def estimate(self, registration_data: pd.DataFrame, quarter: str) -> pd.DataFrame:\n",
    "        adjusted_registration_data = (1 + self.relative_difference_samples).mul(registration_data[quarter]).round(0).astype('Int64')\n",
    "        return adjusted_registration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_registration_relative_difference_calculator = VehicleRegistrationRelativeDifferenceCalculator(car_van_2011_data,\n",
    "                                                                                                      car_van_2021_data,\n",
    "                                                                                                      vehicle_registrations_data) # Actively choosing not to use interpolated data here\n",
    "\n",
    "relative_difference_data = vehicle_registration_relative_difference_calculator.calculate()\n",
    "\n",
    "relative_difference_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_difference_sample_generator = RelativeDifferenceSampleGenerator(relative_difference_data=relative_difference_data,\n",
    "                                                                         n_samples=1000)\n",
    "\n",
    "relative_difference_samples = relative_difference_sample_generator.generate_samples()\n",
    "\n",
    "relative_difference_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_registration_estimator = VehicleRegistrationEstimator(relative_difference_samples)\n",
    "\n",
    "vehicle_registrations_data_samples = vehicle_registration_estimator.estimate(vehicle_registrations_data_i, '2023 Q1')\n",
    "bev_registrations_data_samples = vehicle_registration_estimator.estimate(bev_registrations_data_i, '2023 Q1')\n",
    "phev_registrations_data_samples = vehicle_registration_estimator.estimate(phev_registrations_data_i, '2023 Q1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_registrations_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev_registrations_data_samples.iloc[:, 1].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../Data/PreProcessed/adjustment_factors.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test = json.load(open('../Data/PreProcessed/preprocessed_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'L'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['car_van_2011'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSOA11NM</th>\n",
       "      <th>cars</th>\n",
       "      <th>households</th>\n",
       "      <th>households_without_cars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City of London 001A</td>\n",
       "      <td>452</td>\n",
       "      <td>876</td>\n",
       "      <td>519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>City of London 001B</td>\n",
       "      <td>441</td>\n",
       "      <td>830</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>City of London 001C</td>\n",
       "      <td>180</td>\n",
       "      <td>817</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City of London 001E</td>\n",
       "      <td>124</td>\n",
       "      <td>467</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barking and Dagenham 016A</td>\n",
       "      <td>496</td>\n",
       "      <td>543</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29658</th>\n",
       "      <td>Cardiff 049E</td>\n",
       "      <td>535</td>\n",
       "      <td>548</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29659</th>\n",
       "      <td>Cardiff 005G</td>\n",
       "      <td>704</td>\n",
       "      <td>643</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29660</th>\n",
       "      <td>Cardiff 006F</td>\n",
       "      <td>1001</td>\n",
       "      <td>637</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29661</th>\n",
       "      <td>Swansea 023E</td>\n",
       "      <td>1127</td>\n",
       "      <td>803</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29662</th>\n",
       "      <td>Swansea 025H</td>\n",
       "      <td>658</td>\n",
       "      <td>836</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29663 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        LSOA11NM  cars  households  households_without_cars\n",
       "0            City of London 001A   452         876                      519\n",
       "1            City of London 001B   441         830                      481\n",
       "2            City of London 001C   180         817                      655\n",
       "3            City of London 001E   124         467                      356\n",
       "4      Barking and Dagenham 016A   496         543                      186\n",
       "...                          ...   ...         ...                      ...\n",
       "29658               Cardiff 049E   535         548                      135\n",
       "29659               Cardiff 005G   704         643                      177\n",
       "29660               Cardiff 006F  1001         637                       73\n",
       "29661               Swansea 023E  1127         803                      105\n",
       "29662               Swansea 025H   658         836                      330\n",
       "\n",
       "[29663 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_json(test['car_van_2011'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating On-Plot Parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccommodationTypeCounter:\n",
    "    def __init__(self, accommodation_type_df, houses_df):\n",
    "        self.df = accommodation_type_df\n",
    "        self.houses = houses_df\n",
    "        self.data = None\n",
    "\n",
    "    def count(self) -> pd.DataFrame:\n",
    "        types_data = [\n",
    "            self._filter_by_type('Detached', 'detached'),\n",
    "            self._filter_by_type('Semi-detached', 'semi_detached'),\n",
    "            self._filter_by_type('Terraced', 'terraced'),\n",
    "            self._filter_by_type('In a purpose-built block of flats or tenement', 'purpose_built_flat'),\n",
    "            self._combine_converted_flats()\n",
    "        ]\n",
    "        concatenated = pd.concat(types_data, axis=1)\n",
    "        # Group by index and calculate mean (for LSOA21CDs that share a LSOA11CD)\n",
    "        mean = concatenated.groupby(concatenated.index).mean()\n",
    "        proportions = mean.div(mean.sum(axis=1), axis=0)\n",
    "        accommodation_type_counts = round(proportions.mul(self.houses['households'], axis=0))\n",
    "\n",
    "        # Need to drop duplicates\n",
    "        self.data = accommodation_type_counts\n",
    "        \n",
    "        return accommodation_type_counts\n",
    "\n",
    "    def _filter_by_type(self, accommodation_type: str, new_name: str) -> pd.Series:\n",
    "        result = self.df[self.df['accommodation_type'] == accommodation_type]['Observation']\n",
    "        result.name = new_name\n",
    "        return result\n",
    "\n",
    "    def _combine_converted_flats(self):\n",
    "        types = [\n",
    "            'Part of a converted or shared house, including bedsits',\n",
    "            'Part of another converted building, for example, former school, church or warehouse',\n",
    "            'In a commercial building, for example, in an office building, hotel or over a shop'\n",
    "        ]\n",
    "        return sum([self._filter_by_type(atype, 'converted_flat') for atype in types])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accommodation_type_counter = AccommodationTypeCounter(accomodation_type_2021_data, house_2021_data)\n",
    "accommodation_type_counts = accommodation_type_counter.count()\n",
    "accommodation_type_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnPlotParkingEstimator:\n",
    "    def __init__(self, accommodation_type_counts: pd.DataFrame, n_samples: int):\n",
    "        self.accommodation_type_counts = accommodation_type_counts\n",
    "        self.n_samples = n_samples\n",
    "        self.dict = {\n",
    "            'end_terraced': 0.505,\n",
    "            'mid_terraced': 0.338,\n",
    "            'semi_detached': 0.822,\n",
    "            'detached': 0.961,\n",
    "            'converted_flat': 0.289,\n",
    "            'purpose_built_flat': 0.256\n",
    "            }\n",
    "        self.dict['terraced'] = round(0.3765*self.dict['end_terraced'] + 0.6235*self.dict['mid_terraced'], 3)\n",
    "        del self.dict['end_terraced']\n",
    "        del self.dict['mid_terraced']\n",
    "        self.series = pd.Series(self.dict)\n",
    "        self.on_plot_parking_samples = None\n",
    "    \n",
    "    def estimate(self):\n",
    "        # Convert series values to a 2D array with matching shape\n",
    "        probabilities = self.series[self.accommodation_type_counts.columns].values[np.newaxis, :]\n",
    "        \n",
    "        # Generate samples for each accommodation type and LSOA using broadcasting\n",
    "        samples = binom.rvs(n=self.accommodation_type_counts.values.astype(int),\n",
    "                            p=probabilities,\n",
    "                            size=(self.n_samples, *self.accommodation_type_counts.shape))\n",
    "        \n",
    "        # Sum across accommodation types (axis 2)\n",
    "        summed_samples = samples.sum(axis=2)\n",
    "        \n",
    "        # Convert summed_samples to a DataFrame with appropriate columns and index\n",
    "        on_plot_parking_df = pd.DataFrame(summed_samples, columns=self.accommodation_type_counts.index)\n",
    "        \n",
    "        self.on_plot_parking_samples = on_plot_parking_df\n",
    "        return on_plot_parking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_plot_parking_estimator = OnPlotParkingEstimator(accommodation_type_counts=accommodation_type_counts,\n",
    "                                                   n_samples=1000)\n",
    "on_plot_parking_samples = on_plot_parking_estimator.estimate()\n",
    "on_plot_parking_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehiclesWithOnPlotParkingEstimator:\n",
    "    def __init__(\n",
    "            self, \n",
    "            on_plot_parking_samples,\n",
    "            car_van_2021_data,\n",
    "            car_van_2011_data,\n",
    "            house_data,\n",
    "            # relative_difference_samples,\n",
    "            # vehicle_registrations_data,\n",
    "            ) -> None:\n",
    "        \n",
    "        self.on_plot_parking_samples = on_plot_parking_samples\n",
    "        self.car_van_2021_data = car_van_2021_data\n",
    "        self.car_van_2011_data = car_van_2011_data\n",
    "        self.house_data = house_data\n",
    "        # self.relative_difference_samples = relative_difference_samples\n",
    "        # self.vehicle_registrations_data = vehicle_registrations_data\n",
    "        \n",
    "        self.houses_with_cars = None\n",
    "        self.cars_per_house_with_car = None\n",
    "        self.vehicles_with_on_plot_parking_samples = None\n",
    "        self.proportion_of_vehicles_with_on_plot_parking = None\n",
    "\n",
    "    def estimate(self, vehicle_registrations_data_samples: pd.DataFrame):\n",
    "        \"\"\" \n",
    "        This function assumes that all on-plot parking spaces are filled before\n",
    "        off-plot parking is used.\n",
    "        \"\"\"\n",
    "        self._estimate_houses_with_cars()\n",
    "        self._estimate_proportion_of_vehicles_with_on_plot_parking(vehicle_registrations_data_samples)\n",
    "        return self.vehicles_with_on_plot_parking_samples\n",
    "    \n",
    "    def _estimate_houses_with_cars(self):\n",
    "        cars = self.car_van_2021_data['cars']\n",
    "        houses = self.house_data['households']\n",
    "        houses_without_cars = self.car_van_2021_data['houses_without_cars']\n",
    "        \n",
    "        houses_with_cars = houses - houses_without_cars\n",
    "        self.cars_per_house_with_car = cars / houses_with_cars\n",
    "        self.houses_with_cars = houses_with_cars\n",
    "    \n",
    "    def _estimate_proportion_of_vehicles_with_on_plot_parking(self, vehicle_registrations_data_samples: pd.DataFrame):\n",
    "        self.vehicles_with_on_plot_parking_samples = pd.DataFrame(\n",
    "            np.minimum(vehicle_registrations_data_samples.values, self.on_plot_parking_samples.values), # Assumes same dimensions\n",
    "            index=vehicle_registrations_data_samples.index, \n",
    "            columns=vehicle_registrations_data_samples.columns\n",
    "            )\n",
    "        self.proportion_of_vehicles_with_on_plot_parking = self.vehicles_with_on_plot_parking_samples.divide(vehicle_registrations_data_samples, fill_value=0).replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_with_on_plot_parking_estimator = VehiclesWithOnPlotParkingEstimator(on_plot_parking_samples=on_plot_parking_samples,\n",
    "                                                                             car_van_2011_data=car_van_2011_data,\n",
    "                                                                             car_van_2021_data=car_van_2021_data,\n",
    "                                                                             house_data=house_2021_data)\n",
    "\n",
    "vehicles_with_on_plot_parking_samples = vehicles_with_on_plot_parking_estimator.estimate(vehicle_registrations_data_samples=vehicle_registrations_data_samples)\n",
    "vehicles_with_on_plot_parking_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_of_vehicles_with_on_plot_parking_samples = vehicles_with_on_plot_parking_estimator.proportion_of_vehicles_with_on_plot_parking\n",
    "proportion_of_vehicles_with_on_plot_parking_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_with_on_plot_parking_estimator.vehicles_with_on_plot_parking_samples['E01014375'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_with_on_plot_parking_estimator.proportion_of_vehicles_with_on_plot_parking['E01014372'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVsWithOnPlotParkingEstimator:\n",
    "    def __init__(\n",
    "            self, \n",
    "            # ev_registrations_data: pd.DataFrame,\n",
    "            proportion_of_vehicles_with_on_plot_parking: pd.DataFrame\n",
    "            ) -> None:\n",
    "        self.vehicle_registrations_data_samples = vehicle_registrations_data_samples\n",
    "        # self.ev_registrations_data = ev_registrations_data\n",
    "        self.relative_difference_samples = relative_difference_samples\n",
    "        self.proportion_of_vehicles_with_on_plot_parking = proportion_of_vehicles_with_on_plot_parking\n",
    "\n",
    "        self.proportion_of_evs_with_on_plot_parking = None\n",
    "        self.evs_with_on_plot_parking_samples = None\n",
    "\n",
    "    def estimate(self, ev_registrations_data_samples: pd.DataFrame):\n",
    "        self._count_evs_with_on_plot_parking(ev_registrations_data_samples)\n",
    "        return self.evs_with_on_plot_parking_samples\n",
    "\n",
    "    def _calculate_proportion_of_evs_with_on_plot_parking(self, mean_total_vehicles, ev_registrations_data_samples):\n",
    "        # Define alpha and beta values for beta distribution that describes likely starting points for EV on-plot parking access\n",
    "        mu = 0.9 # prior mean\n",
    "        var = 0.1 * mu*(1-mu)\n",
    "        alpha = mu*(mu*(1-mu)/var - 1)\n",
    "        beta = (1-mu)*(mu*(1-mu)/var - 1)\n",
    "\n",
    "        c = stats.beta.rvs(a=alpha, b=beta, size=(1000, len(self.proportion_of_vehicles_with_on_plot_parking.columns))) # y intercept\n",
    "        m = (self.proportion_of_vehicles_with_on_plot_parking - c)/mean_total_vehicles # gradient\n",
    "        p = m*ev_registrations_data_samples + c\n",
    "        return np.clip(p, a_min=0, a_max=1)\n",
    "        \n",
    "    def _count_evs_with_on_plot_parking(self, ev_registrations_data_samples: pd.DataFrame):\n",
    "        # Return proportion of EVs with on-plot parking access. \n",
    "        # This changes depending on how many EVs there are relative to the total number of vehicles\n",
    "        # It approaches the overall proportion for all vehicles.\n",
    "        \n",
    "        # adjusted_vehicle_registration_data_samples = (1 + self.relative_difference_samples).mul(self.vehicle_registrations_data[quarter], axis=1)\n",
    "        mean_total_vehicles = self.vehicle_registrations_data_samples.mean()\n",
    "        # adjusted_ev_registrations_data_samples = (1 + self.relative_difference_samples).mul(ev_registrations_data[quarter], axis=1)\n",
    "        \n",
    "        self.proportion_of_evs_with_on_plot_parking = self._calculate_proportion_of_evs_with_on_plot_parking(\n",
    "            mean_total_vehicles, \n",
    "            ev_registrations_data_samples.sample(frac=1) # to change the order of the relative differences otherwise they cancel each other out in the next step\n",
    "            )\n",
    "        self.evs_with_on_plot_parking_samples = self.proportion_of_evs_with_on_plot_parking.mul(ev_registrations_data_samples, axis=1) # .round(0)\n",
    "        return self.evs_with_on_plot_parking_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evs_with_on_plot_parking_estimator = EVsWithOnPlotParkingEstimator(proportion_of_vehicles_with_on_plot_parking=proportion_of_vehicles_with_on_plot_parking_samples)\n",
    "\n",
    "bevs_with_on_plot_parking_samples = evs_with_on_plot_parking_estimator.estimate(bev_registrations_data_samples)\n",
    "bevs_with_on_plot_parking_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bevs_with_on_plot_parking_samples['E01014374'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bevs_with_on_plot_parking_samples['E01014374'].plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Distributions to the Probabilistic Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm, lognorm, gamma, beta\n",
    "\n",
    "data = bevs_with_on_plot_parking_samples['E01014374']\n",
    "\n",
    "params_skewnorm = skewnorm.fit(data)\n",
    "params_lognorm = lognorm.fit(data)\n",
    "params_gamma = gamma.fit(data)\n",
    "params_beta = beta.fit(data)\n",
    "\n",
    "plt.hist(data, bins=50, density=True, alpha=0.6, color='g', label='Original Data')\n",
    "\n",
    "# Create an array over which you'll compute the PDFs\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "# Plot the PDFs using the parameters obtained from the fit\n",
    "plt.plot(x, skewnorm.pdf(x, *params_skewnorm), 'k', linewidth=2, label='Fit: SkewNorm')\n",
    "plt.plot(x, lognorm.pdf(x, *params_lognorm), 'r', linewidth=2, label='Fit: LogNorm')\n",
    "plt.plot(x, gamma.pdf(x, *params_gamma), 'y', linewidth=2, label='Fit: Gamma')\n",
    "plt.plot(x, beta.pdf(x, *params_beta), 'b', linewidth=2, label='Fit: Beta')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Data Values')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Original Data and Fitted Distributions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "D_skewnorm, p_skewnorm = kstest(data, 'skewnorm', params_skewnorm)\n",
    "D_lognorm, p_lognorm = kstest(data, 'lognorm', params_lognorm)\n",
    "D_gamma, p_gamma = kstest(data, 'gamma', params_gamma)\n",
    "D_beta, p_beta = kstest(data, 'beta', params_beta)\n",
    "\n",
    "print(f\"SkewNorm: D={D_skewnorm}, p={p_skewnorm}\")\n",
    "print(f\"LogNorm: D={D_lognorm}, p={p_lognorm}\")\n",
    "print(f\"Gamma: D={D_gamma}, p={p_gamma}\")\n",
    "print(f\"Beta: D={D_beta}, p={p_beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CramÃ©râ€“von Mises statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "def cramer_von_mises_skewnorm(data, params):\n",
    "    data_sorted = np.sort(data)\n",
    "    N = len(data_sorted)\n",
    "    \n",
    "    # Expected CDF values under the skewed normal distribution\n",
    "    cdf_theoretical = skewnorm.cdf(data_sorted, *params)\n",
    "    \n",
    "    # Observed ECDF values\n",
    "    i = np.arange(1, N+1)\n",
    "    ecdf_observed = i / N\n",
    "    \n",
    "    # Calculate CramÃ©râ€“von Mises Statistic\n",
    "    W2 = 1/12 + np.sum((ecdf_observed - cdf_theoretical)**2)\n",
    "    \n",
    "    return W2\n",
    "\n",
    "params_skewnorm = skewnorm.fit(data)\n",
    "\n",
    "W2 = cramer_von_mises_skewnorm(data, params_skewnorm)\n",
    "print(f\"CramÃ©râ€“von Mises Statistic for Skewed Normal: {W2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skewnorm, cramervonmises\n",
    "\n",
    "def cramer_von_mises_skewnorm(data):\n",
    "    # Estimate the parameters of the skewed normal distribution\n",
    "    a, loc, scale = skewnorm.fit(data)\n",
    "\n",
    "    # Apply the CramÃ©râ€“von Mises test\n",
    "    result = cramervonmises(data, skewnorm.cdf, args=(a, loc, scale))\n",
    "\n",
    "    return result\n",
    "\n",
    "data = bevs_with_on_plot_parking_samples['E01014374']\n",
    "\n",
    "cvm_result = cramer_von_mises_skewnorm(data)\n",
    "print(cvm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Q Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skewnorm, probplot\n",
    "\n",
    "# Using probplot with the skewed normal distribution\n",
    "osm, osr = probplot(data, dist=skewnorm, sparams=skewnorm.fit(data), plot=plt)\n",
    "\n",
    "plt.title(\"Q-Q plot for Skewed Normal distribution\")\n",
    "plt.xlabel(\"Theoretical Quantiles\")\n",
    "plt.ylabel(\"Observed Quantiles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_registrations_data_samples['E01000119']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating LSOA Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSOA:\n",
    "    def __init__(self) -> None:\n",
    "        self.name: str\n",
    "        self.gis_data: GISData\n",
    "        self.params: LSOAParams\n",
    "\n",
    "class GISData:\n",
    "    def __init__(self) -> None:\n",
    "        self.latitude:float\n",
    "        self.longitude:float\n",
    "        self.boundaries:list[GISBoundary]\n",
    "\n",
    "class GISBoundary:\n",
    "    def __init__(self) -> None:\n",
    "        self.latitudes:list[float]\n",
    "        self.longitudes:list[float]\n",
    "\n",
    "class LSOAParams:\n",
    "    def __init__(self) -> None:\n",
    "        self.households: str\n",
    "        self.accommodation_type: dict\n",
    "        self.on_plot_parking: DataValue\n",
    "        self.vehicles: DataValue\n",
    "        self.bevs: DataValue\n",
    "        self.phevs: DataValue\n",
    "        self.bevs_with_on_plot_parking: DataValue\n",
    "        self.phevs_with_on_plot_parking: DataValue\n",
    "\n",
    "class DataValue:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean: float\n",
    "        self.std: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping LSOAs Information to Selected Distribution Substations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LSOA Boundary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_boundaries = gpd.read_file('../Data/Spatial/LSOA/LSOA_2011_EW_BFC_V3_WGS84/LSOA_2011_EW_BFC_V3_WGS84.shp')\n",
    "lsoa_boundaries = lsoa_boundaries.set_index('LSOA11CD')\n",
    "lsoa_boundaries = lsoa_boundaries.loc[common_lsoas].sort_index()\n",
    "lsoa_boundaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Distribution Substation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    pd.read_csv('../Data/DistributionNetwork/distribution_substations.csv')\n",
    "    .drop(\n",
    "        columns = [\n",
    "            'Transformer Headroom', 'LCT Count Total', 'Energy Storage', 'Heat Pumps', \n",
    "            'Total LCT Capacity', 'Total Generation Capacity', 'Solar', \n",
    "            'Wind', 'Bio Fuels', 'Water Generation', 'Waste Generation',\n",
    "            'Storage Generation', 'Fossil Fuels', 'Other Generation']\n",
    "    )\n",
    "    .replace('Hidden', np.nan)\n",
    "    .astype({'Customers':'float64', 'Substation Number':'Int64'})\n",
    "    .astype({'Substation Number': str})\n",
    ")\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_geo = (\n",
    "    gpd.read_file('../Data/DistributionNetwork/dist_swest_march2023.gpkg')\n",
    "    .rename(columns={'NR':'Substation Number'})\n",
    "    .dissolve('Substation Number').reset_index()\n",
    "    .merge(ds, how='left', on ='Substation Number')\n",
    "    .rename(columns={'Substation Name':'Name'})\n",
    "    .fillna(value={'Discount':'Unknown'}) # For the \"key_on\" part of the choropleth map\n",
    "    .to_crs('EPSG:4326')\n",
    ")\n",
    "\n",
    "ds_geo['Location'] = gpd.GeoSeries(gpd.points_from_xy(ds_geo.LONGITUDE, ds_geo.LATITUDE, crs=\"EPSG:4326\"))\n",
    "\n",
    "ds_geo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Data Compression / Data Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jittered_data = ds_bevs + np.random.uniform(0, 1, size=ds_bevs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ds_bevs, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(jittered_data, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ds_bevs, bins=len(np.unique(ds_bevs))-1, density=True, alpha=0.6, color='b', label='Original Data')\n",
    "plt.hist(jittered_data, bins=50, density=True, alpha=0.6, color='g', label='Jittered Data')\n",
    "\n",
    "params_skewnorm = skewnorm.fit(jittered_data)\n",
    "\n",
    "# Create an array over which you'll compute the PDFs\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "# Plot the PDFs using the parameters obtained from the fit\n",
    "plt.plot(x, skewnorm.pdf(x, *params_skewnorm), 'k', linewidth=2, label='Fit: SkewNorm')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Data Values')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Original Data and Fitted Distributions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples you wish to generate\n",
    "num_samples = 1000\n",
    "\n",
    "samples = skewnorm.rvs(*params_skewnorm, size=num_samples).round(0).astype(int)\n",
    "\n",
    "# Plot histogram to visualize the skewed distribution\n",
    "plt.hist(ds_bevs, bins=50, density=True, alpha=0.4, color='b', label='Original Data')\n",
    "plt.hist(samples, bins=60, density=True, alpha=0.4, color='g', label='Re-sampled Data')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Histogram of Skewed Normal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewnorm.fit(jittered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bevs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "# Parameter\n",
    "lambda_val = ds_bevs.mean()  # average rate of value\n",
    "\n",
    "# Values\n",
    "x = np.arange(0, 2*lambda_val+1)  # typically sufficient range for plotting\n",
    "y = poisson.pmf(x, lambda_val)\n",
    "\n",
    "# Plot\n",
    "plt.bar(x, y, align='center', alpha=0.7)\n",
    "plt.xlabel('Number of events')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Poisson Distribution\\n lambda={lambda_val}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentiles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 5% percentile bands\n",
    "percentiles = [np.percentile(ds_bevs, p).astype(int) for p in range(0, 101, 5)]\n",
    "\n",
    "# Convert to pandas Series\n",
    "percentile_series = pd.Series(percentiles, index=[f\"{i}%\" for i in range(0, 101, 5)])\n",
    "\n",
    "percentile_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_data = []\n",
    "for i in range(len(percentile_series) - 1):\n",
    "    start = percentile_series.iloc[i]\n",
    "    end = percentile_series.iloc[i+1]\n",
    "    # Number of points for 5% of the data\n",
    "    num_points = int(0.005 * len(ds_bevs))\n",
    "    reconstructed_data.extend(np.linspace(start, end, num_points))\n",
    "\n",
    "reconstructed_data = np.array(reconstructed_data).round(0).astype(int)\n",
    "reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ds_bevs, bins=50, color='g', alpha=0.4, label='Original Data', density=True)\n",
    "plt.hist(reconstructed_data, bins=50, color='b', alpha=0.4, label='Reconstructed Data', density=True)\n",
    "plt.legend(loc='best')\n",
    "plt.title('A comparison between original data and \\n data reconstructed from the percentiles.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Probabilities (normalised value counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values and their counts\n",
    "values, counts = np.unique(ds_bevs, return_counts=True)\n",
    "\n",
    "# Convert to Pandas Series\n",
    "counts = pd.Series(counts, index=values)\n",
    "probabilities = counts / counts.sum()\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.plot.bar(alpha=0.5)\n",
    "plt.xticks(rotation='horizontal')\n",
    "plt.xlabel('Number of BEVs')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a list of substation objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DistSubstation:\n",
    "    def __init__(self) -> None:\n",
    "        self.name:str\n",
    "        self.id:str # str instead of int (for now)\n",
    "        self.gisData:GISData\n",
    "        self.vehicles:Vehicles\n",
    "        self.evDemands:EVDemands\n",
    "        self.params:SubstationParams\n",
    "\n",
    "class SubstationParams:\n",
    "    def __init__(self) -> None:\n",
    "        self.numCustomers:int\n",
    "        self.parentLSOAs:list\n",
    "\n",
    "class GISData:\n",
    "    def __init__(self) -> None:\n",
    "        self.latitude:float\n",
    "        self.longitude:float\n",
    "        self.boundaries:gpd.GeoSeries\n",
    "\n",
    "class Vehicles:\n",
    "    def __init__(self) -> None:\n",
    "        self.vehicles:pd.Series\n",
    "        self.bevs:pd.Series\n",
    "        self.phevs:pd.Series\n",
    "        self.bevsWithOnPlotParking:pd.Series\n",
    "        self.phevsWithOnPlotParking:pd.Series\n",
    "\n",
    "class EVDemands:\n",
    "    def __init__(self) -> None:\n",
    "        self.evDemandList:list[EVDemand]\n",
    "\n",
    "class Quarter(Enum):\n",
    "    Q1=0\n",
    "    Q2=1\n",
    "    Q3=2\n",
    "    Q4=3\n",
    "\n",
    "class EVDemand:\n",
    "    def __init__(self) -> None:\n",
    "        self.bev:DataValue\n",
    "        self.phev:DataValue\n",
    "        self.year:int\n",
    "        self.quarter:Quarter\n",
    "\n",
    "class DataValue:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean:float = 0\n",
    "        self.std:float = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping this in a SubstationDataMapper Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class should recieve a list of substation objects, and return them with the empty attributes filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Multiple Substation Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_substation_objects(ds_geo, substation_numbers):\n",
    "    substations = []\n",
    "    for substation_number in substation_numbers:\n",
    "        substation = DistSubstation()\n",
    "        data = ds_geo[ds_geo['Substation Number'] == substation_number]\n",
    "        substation.name = data['Name'].values[0]\n",
    "        substation.id = data['Substation Number'].values[0]\n",
    "\n",
    "        gis_data = GISData()\n",
    "        gis_data.latitude = data['LATITUDE'].values[0]\n",
    "        gis_data.longitude = data['LONGITUDE'].values[0]\n",
    "        gis_data.boundaries = data['geometry'].values[0]\n",
    "        substation.gisData = gis_data\n",
    "\n",
    "        params = SubstationParams()\n",
    "        params.numCustomers = data['Customers'].values[0]\n",
    "        params.parentLSOAs = None\n",
    "        substation.params = params\n",
    "        substations.append(substation)\n",
    "\n",
    "    return substations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSubstationDataMapper:\n",
    "    SAMPLE_SIZE = 1000\n",
    "    PERCENTILE_INCREMENT = 5\n",
    "\n",
    "    def __init__(self, ds_data: pd.DataFrame, lsoa_boundaries: gpd.GeoDataFrame, house_data: pd.DataFrame) -> None:\n",
    "        self.ds_data = ds_data.set_index('Substation Number')\n",
    "        self.lsoa_boundaries = lsoa_boundaries\n",
    "        self.house_data = house_data\n",
    "\n",
    "    def map_to_substation(self, substations: object, data: dict):\n",
    "        for substation in substations:\n",
    "            parent_lsoas, intersections = self._find_parent_lsoas(substation)\n",
    "            substation.params.parentLSOAs = parent_lsoas\n",
    "            vehicles_instance = Vehicles()\n",
    "            for key in data:\n",
    "                setattr(vehicles_instance, key, self._allocate_data_from_lsoa_to_ds(data[key], substation, parent_lsoas, intersections))\n",
    "            substation.vehicles = vehicles_instance\n",
    "\n",
    "    def _find_parent_lsoas(self, substation: object):\n",
    "        intersections = self.lsoa_boundaries.geometry.intersection(self.ds_data.loc[substation.id].geometry)\n",
    "        pip_mask = ~intersections.is_empty\n",
    "        parent_lsoas = self.lsoa_boundaries[pip_mask].index.values\n",
    "        return parent_lsoas, intersections\n",
    "\n",
    "    def _allocate_data_from_lsoa_to_ds(self, data: pd.DataFrame, substation: object, parent_lsoas: list, intersections):\n",
    "        data_filtered = data[parent_lsoas]\n",
    "        household_intersection = self._calculate_household_intersection(substation, parent_lsoas, intersections)\n",
    "        data_from_lsoas = np.empty(shape=(len(parent_lsoas), self.SAMPLE_SIZE))\n",
    "        for i, lsoa in enumerate(parent_lsoas):  # For each intersecting LSOA\n",
    "            n = np.maximum(data_filtered[lsoa].astype(int), 0)\n",
    "            p = np.clip(household_intersection.loc[lsoa], 0, 1)\n",
    "            data_from_lsoas[i] = binom.rvs(n=n, p=p, size=(1, self.SAMPLE_SIZE))\n",
    "        data_from_lsoas = np.add.reduce(data_from_lsoas).flatten().astype(int)\n",
    "        return self._calculate_percentiles(data=data_from_lsoas)\n",
    "\n",
    "    def _calculate_household_intersection(self, substation: object, parent_lsoas: list, intersections):\n",
    "        ds_customers_in_lsoas = self._calculate_ds_customers_in_lsoas(substation, parent_lsoas, intersections)\n",
    "        households = self.house_data.loc[parent_lsoas].households\n",
    "        return ds_customers_in_lsoas.divide(households)\n",
    "\n",
    "    def _calculate_ds_customers_in_lsoas(self, substation: object, parent_lsoas: list, intersections):\n",
    "        relative_intersections = self._calculate_relative_intersection(substation, parent_lsoas, intersections)\n",
    "        ds_customers_in_lsoas = relative_intersections * substation.params.numCustomers\n",
    "        ds_customers_in_lsoas = ds_customers_in_lsoas.fillna(0) # If numCustomers == NaN, assume numCustomers = 0\n",
    "        return ds_customers_in_lsoas # Can sometimes be NaN which causes problems. Need to code an alternative way to calulate based purely on area intersection!\n",
    "\n",
    "    def _calculate_relative_intersection(self, substation: object, parent_lsoas: list, intersections):\n",
    "        intersection_areas = intersections.loc[parent_lsoas].area\n",
    "        substation_area = self.ds_data.loc[substation.id].geometry.area\n",
    "        return intersection_areas / substation_area\n",
    "\n",
    "    def _calculate_percentiles(self, data):\n",
    "        percentiles = [np.percentile(data, p).astype(int) for p in range(0, 101, self.PERCENTILE_INCREMENT)]\n",
    "        percentile_series = pd.Series(percentiles, index=[f\"{i}%\" for i in range(0, 101, self.PERCENTILE_INCREMENT)])\n",
    "        return percentile_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = '2023 Q1'\n",
    "\n",
    "# Vehicle Registrations\n",
    "vehicle_registration_estimator = VehicleRegistrationEstimator(relative_difference_samples)\n",
    "vehicle_registrations_data_samples = vehicle_registration_estimator.estimate(vehicle_registrations_data_i, quarter)\n",
    "bev_registrations_data_samples = vehicle_registration_estimator.estimate(bev_registrations_data_i, quarter)\n",
    "phev_registrations_data_samples = vehicle_registration_estimator.estimate(phev_registrations_data_i, quarter)\n",
    "\n",
    "# EVs with On Plot Parking\n",
    "evs_with_on_plot_parking_estimator = EVsWithOnPlotParkingEstimator(proportion_of_vehicles_with_on_plot_parking_samples)\n",
    "bevs_with_on_plot_parking_samples = evs_with_on_plot_parking_estimator.estimate(bev_registrations_data_samples)\n",
    "phevs_with_on_plot_parking_samples = evs_with_on_plot_parking_estimator.estimate(phev_registrations_data_samples)\n",
    "\n",
    "# proportion_of_vehicles_with_on_plot_parking_samples is assumed to be stationary and can therefore be precomputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations = create_substation_objects(ds_geo, ds_geo['Substation Number'].sample(100).values)\n",
    "\n",
    "substation_data_mapper = MultiSubstationDataMapper(\n",
    "    ds_data=ds_geo,\n",
    "    lsoa_boundaries=lsoa_boundaries,\n",
    "    house_data=house_2021_data\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'vehicles': vehicle_registrations_data_samples, \n",
    "    'bevs': bev_registrations_data_samples,\n",
    "    'phevs': phev_registrations_data_samples,\n",
    "    'bevsWithOnPlotParking': bevs_with_on_plot_parking_samples,\n",
    "    'phevsWithOnPlotParking': phevs_with_on_plot_parking_samples\n",
    "}\n",
    "\n",
    "substation_data_mapper.map_to_substation(substations=substations, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations[2].vehicles.vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations[2].vehicles.bevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations[2].vehicles.bevsWithOnPlotParking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations[2].params.parentLSOAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev_registrations_data_samples[substations[2].params.parentLSOAs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for evidence in real world monitor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substation_list = list(pd.read_csv('../Data/DistributionNetwork/substation_list.csv', header=None, dtype=str).sort_values(by=0, ascending=True).iloc[:, 0].values)\n",
    "substation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds_geo['Substation Number'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_substation_list = intersection_of_lists(substation_list, list(ds_geo['Substation Number'].values))\n",
    "common_substation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_substation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substations = create_substation_objects(ds_geo, common_substation_list)\n",
    "\n",
    "substation_data_mapper = MultiSubstationDataMapper(\n",
    "    ds_data=ds_geo,\n",
    "    lsoa_boundaries=lsoa_boundaries,\n",
    "    house_data=house_2021_data\n",
    ")\n",
    "\n",
    "data = {\n",
    "    'vehicles': vehicle_registrations_data_samples, \n",
    "    'bevs': bev_registrations_data_samples,\n",
    "    'phevs': phev_registrations_data_samples,\n",
    "    'bevsWithOnPlotParking': bevs_with_on_plot_parking_samples,\n",
    "    'phevsWithOnPlotParking': phevs_with_on_plot_parking_samples\n",
    "}\n",
    "\n",
    "substation_data_mapper.map_to_substation(substations=substations, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{i}%\" for i in range(0, 101, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bevs_with_on_plot_parking_selected_substations = pd.DataFrame(index=[f\"{i}%\" for i in range(0, 101, 5)], columns=common_substation_list)\n",
    "for substation in substations:\n",
    "    bevs_with_on_plot_parking_selected_substations[substation.id] = substation.vehicles.bevsWithOnPlotParking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bevs_with_on_plot_parking_selected_substations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bevs_with_on_plot_parking_selected_substations.to_csv('../Data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probabilistic_ev_demand_modelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
